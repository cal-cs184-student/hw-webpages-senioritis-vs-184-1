<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Ashley Kao</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-senioritis-vs-184-1/">https://cal-cs184-student.github.io/hw-webpages-senioritis-vs-184-1/</a>
		
		<br>
		
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-senioritis-vs-184-3">https://github.com/cal-cs184-student/sp25-hw3-senioritis-vs-184-3</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Homework 3 adds path tracing to the renderer, incorporating functionality such as ray generation, scene intersection, acceleration structures,
		direct illumination, global illumination, and adaptive sampling.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<!-- Walk through ray generation and primitive intersection parts of the rendering pipeline -->
		 Ray generation involves taking in normalized x and y coordinates in image space and transforming them into camera space.
		 As the image is mapped to the virtual camera sensor, the relative positions of the input arguments relative to the normalized width and height of the image
		 will determine their relative positions on the sensor. Once these values have been calculated, a ray is generated with direction equal to the bottom left corner
		 of the sensor plus the relative positions of x and y in the sensor. This is done because we have the offset of the arguments from the bottom-left corner
		 but we need to incorporate the position of the sensor itself. The position of the camera is already given in world space so we just transform the direction vector into world space.
		 We then use the camera position and normalized direction vector in world space to create the ray that will be returned. Before doing so, we update the min_t and max_t of the newly created ray.

		 To generate estimate the radiance over a pixel, we generate ns_aa pixel samples over the pixel corresponding to the passed-in x and y values.
		 These samples are used to generate sample rays, whose estimated global illuminations are summed up and averaged to generate the Monte Carlo estimate of the pixel's value.

		<!-- Explain the triangle intersection alg you implemented in your own words -->
		 The second half of Part 1 covers ray-object intersections. The first is Ray-Triangle intersection. My implementation
		 of ray-triangle intersection is essentially the Moller-Trumbore algorithm. I first calculate E1, E2, S, S1, and S2.
		 Then I use the Moller-Trumbore algorithm to obtain the time t of intersection along with two of the three values for barycentric interpolation.
		 Using the two values to calculate the third and incorporating the vertices of the triangle gave me the intersection point.
		 There were two methods that I implemented for ray-triangle intersection. Both return a boolean indicating whether an intersection exists
		 and update the max_t of the ray but only one of them updates an Intersection object (and does another barycentric interpolation on the vertex normals in the process).

		 The second ray-object intersection is ray-sphere intersection. This time I used the formula from the lecture slides and solved the quadratic equation to get the intersection time(s) if they exist.
		 I update max_t to be the smaller of the times, bounded by min_t and max_t. As with ray-triangle intersection, implementing this intersection required implementing two methods
		 in which ony one updates an Intersection object. This intersection did not require interpolation as the normal could be calculated from the sphere center and intersection point. Additionally,
		 I used a helper method to determine whether a valid intersection existed, and if so, update max_t and determine the min intersection time.

		<!-- Show images with normal shading for a few small .dae files -->
		 According to Ed, in order to have normal shading we needed to modify PathTracer::est_radiance_global_illumination(cost Ray &r).
		 However, I compared the code on my end against the code block on Ed and they were identical so I didn't end up changing anything.

		 <p>Here are some images with normal shading generated from small .dae files.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="CBtest-cube.png" width="400px"/>
				  <figcaption>simple/cube.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBtest-coil.png" width="400px"/>
				  <figcaption>sky/CBcoil.dae</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="CBtest-banana.png" width="400px"/>
				  <figcaption>keenan/banana.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBtest-bunny.png" width="400px"/>
				  <figcaption>sky/bunny.dae</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		My BVH construction algorithm first constructs the bounding box of all primitives in the node. As my BVH's heuristic is average centroid,
		I count up the number of primitives in the node and find the average centroid of these primitives at the same time. I then create a new node with 
		the bounding box of all node primitives. If there are fewer than max_leaf_size primitives in the current node, we know our new node is a leaf node, 
		so we set its start and end pointers with the start and end arguments passed in to the method. If there are > max_leaf_size primitives,
		we need to find the left and right child of the node. In order to do so, we must split the primitives into two groups (one for the left subtree and one for the right subtree).
		I choose the axis to split on based on which of x, y, or z of the overall bounding box is the largest. Once this is found, I compare the centroid of 
		each primitive's bounding boxes. If the centroid[axis] is less than the splitting value, the primitive will go into the left subtree. Otherwise it goes into the right.
		Finally I call the BVH construction method recursively with each list to get the left and right children. 

		<p>Here are some images with normal shading generated from large .dae files.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part2-lucy.png" width="400px"/>
				  <figcaption>sky/CBlucy.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2-beast.png" width="400px"/>
				  <figcaption>meshedit/beast.dae</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part2-dragon.png" width="400px"/>
				  <figcaption>sky/CBdragon.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2-blob.png" width="400px"/>
				  <figcaption>sky/blob.dae</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		There were noticeable speedups when BVH was used versus when it wasn't. Larger files would frequently take minutes
		to render without the BVH, whereas they would only take less than a tenth of a second with the BVH. Exact time differences
		for four different medium-sized scenes can be seen in the table below. This happens because in the non-BVH implementation,
		the code must iterate through every primitive in the singular node, which is very costly when there are a lot of primitives (such as in large meshes).
		The benefit of a BVH is that only leaf nodes hold a small group of primitives and subtrees that the ray doesn't intersect are skipped entirely,
		so significantly fewer primitives are traversed. The scene is thus able to be rendered much more quickly.
		<p>Here are some images with normal shading generated from large .dae files.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="part2-banana-nobvh.png" width="400px"/>
				  <figcaption>keenan/banana.dae without BVH</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2-banana-bvh.png" width="400px"/>
				  <figcaption>keenan/banana.dae with BVH</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part2-cow-nobvh.png" width="400px"/>
				  <figcaption>meshedit/cow.dae without BVH</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2-cow-bvh.png" width="400px"/>
				  <figcaption>meshedit/cow.dae with BVH</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part2-coil-nobvh.png" width="400px"/>
				  <figcaption>sky/CBcoil.dae without BVH</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2-coil-bvh.png" width="400px"/>
				  <figcaption>sky/CBcoil.dae with BVH</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="part2-cbbunny-nobvh.png" width="400px"/>
				  <figcaption>sky/CBbunny.dae without BVH</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="part2-cbbunny-bvh.png" width="400px"/>
				  <figcaption>sky/CBbunny.dae with BVH</figcaption>
				</td>
			  </tr>
			</table>
		</div>


		<h2>Part 3: Direct Illumination</h2>
		<!-- Walk through both implementations of the direct lighting function -->
		 I first set up the BSDF for diffuse materials as it would be used throughout Part 3. The decision to divide by pi 
		 was made from Discussion 07, where BRDF was given as rho / pi. For direct lighting with uniform hemisphere sampling,
		 I make numLights * samplesPerLight samples total, as we are doing Monte Carlo sampling. For each sample, I sample over the hemisphere to get an object space
		 vector for the input direction vector w_in. I use this w_in to create a ray to see if it intersects a light source. If 
		 it does, I calculate the outgoing light using the reflection equation. At the end, I average the sum of the samples' outgoing light 
		 to get the estimated overall outgoing light.

		 Light sampling differs from hemisphere sampling in that we are sampling from the scene's lights rather than uniformly overa hemisphere.
		 We iterate over the scene's lights, taking one sample if the current light is a point light source and ns_area_light samples otherwise.
		 Calling sample_L on the light gives us the resulting radiance, w_in, distance to the light from the hit point, and the pdf corresponding to the sampled direction.
		 Before we substitute these values into the reflection equation, we need to check whether there is an object between the light and the scene in the sample direction (if 
		 there is, the scene will be in the shadow for this direction and no light will be added). Only once we've verified that there is no intersection 
		 do we calculate the outgoing light for the current sample. Once all samples for the current light have been taken, we average them to get the Monte Carlo 
		 estimate for the light. At the very end once all lights have been iterated over, we sum up the estimates for each light to get the overall estimatated outgoing light.

		<!-- Show images rendered with both implementations of the direct lighting function -->
		<p>Here are some images rendered using the direct lighting functions.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="CBbunny_H_16_8.png" width="400px"/>
				  <figcaption>sky/CBbunny.dae using uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBbunny_16_8.png" width="400px"/>
				  <figcaption>sky/CBbunny.dae with light sampling</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="CBspheresl_H_16_8.png" width="400px"/>
				  <figcaption>sky/CBspheres_lambertian.dae with uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBspheresl_16_8.png" width="400px"/>
				  <figcaption>sky/CBspheres_lambertian.dae with light sampling</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="wall-e_H_16_8.png" width="400px"/>
				  <figcaption>sky/wall-e.dae with uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="wall-e_16_8.png" width="400px"/>
				  <figcaption>sky/wall-e.dae with light sampling</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		Notice that wall-e.dae with uniform hemisphere sampling shows up entirely dark. This is likely because there are no area lights 
		in the scene whereas the other two scenes had area lights.

		<!-- Focus on one particular scene with at least one area light and compare the noise levels in soft shadows
		 when rendering with 1, 4, 16, and 64 light rays, with one sample per pixel and using light sampling -->
		 <p>Here is sky/CBbunny.dae rendered with 1 sample per pixel and varying amounts of light rays.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="CBbunny_1_1.png" width="400px"/>
				  <figcaption>l = 1</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBbunny_1_4.png" width="400px"/>
				  <figcaption>l = 4</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="CBbunny_1_16.png" width="400px"/>
				  <figcaption>l = 16</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="CBbunny_1_64.png" width="400px"/>
				  <figcaption>l = 64</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		
		<!-- Compare the results between uniform hemisphere sampling and lighting sampling in one paragraph -->

		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>